{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d12e530b-a433-4811-b576-acf49e35f2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1290d89b-4b46-4908-8255-739f69ba5db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, lit, to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import * \n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ddc14b7-22bf-4d17-bf7b-5cfda2a138a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fdd0bd0-1214-4c9e-8b23-8ab09a31ae28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point, we are going to transform all the data extracted into Delta Lake format. This approach makes it easier to apply data handling methods like data cleansing and schema enforcement.\n",
    "\n",
    "We are also filtering the ingestion data and making the data loading incremental with the following strategies:\n",
    "\n",
    "- **Dynamic Cutoff Date:** Based on a configurable variable (`ANALYSIS_WINDOW_MONTHS`), fulfilling the project requirement for an automated, sliding historical window (e.g., \"Last 6 months\") that updates automatically with every execution.\n",
    "\n",
    "- **Spark Structured Streaming with Auto Loader (cloudFiles):**  \n",
    "  Instead of standard file reading, we use Databricks **Auto Loader** (`format(\"cloudFiles\")`). This is crucial because:\n",
    "  1. It efficiently detects new files as they land in S3 without listing directories repeatedly.\n",
    "  2. It natively supports various formats (JSON, Avro, etc.) on Shared Clusters where standard streaming sources might be restricted.\n",
    "  3. It handles **Schema Inference and Evolution** robustly.\n",
    "\n",
    "- **Mixed Source Formats:** The code iterates through a configuration map that defines not just the source path and target table, but also the specific **file format** (JSON or Avro) for each dataset.\n",
    "\n",
    "- **S3 Checkpointing:** Checkpoints are stored directly in S3 (`s3://.../checkpoints`) rather than DBFS. This ensures persistence, security, and avoids permissions errors common in shared environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43577d87-a279-4459-ac84-51ab6cbf72ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data importation"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data source paths\n",
    "SOURCE_PATH_1 = \"s3://aurorapay-studycase/customer_profiles\"\n",
    "SOURCE_PATH_2 = \"s3://aurorapay-studycase/device_signals\"\n",
    "SOURCE_PATH_3 = \"s3://aurorapay-studycase/merchant_registry\"\n",
    "SOURCE_PATH_4 = \"s3://aurorapay-studycase/security_logs\"\n",
    "SOURCE_PATH_5 = \"s3://aurorapay-studycase/transaction_events\"\n",
    "\n",
    "# Target tables where data will be stored\n",
    "TARGET_TABLE_1 = \"fraud_detection_project.bronze_layer.customer_profiles\"\n",
    "TARGET_TABLE_2 = \"fraud_detection_project.bronze_layer.device_signals\"\n",
    "TARGET_TABLE_3 = \"fraud_detection_project.bronze_layer.merchant_registry\"\n",
    "TARGET_TABLE_4 = \"fraud_detection_project.bronze_layer.security_logs\"\n",
    "TARGET_TABLE_5 = \"fraud_detection_project.bronze_layer.transaction_events\"\n",
    "# -----------------------------------------\n",
    "# Definition of the Analysis Window (in months)\n",
    "# This is the period we want to analyze for fraud detection.\n",
    "ANALYSIS_WINDOW_MONTHS = 6\n",
    "\n",
    "# Atuomatic calculation of the cutoff date\n",
    "# If the job runs in 3 weeks, this date advances together, keeping the moving window\n",
    "cutoff_date = datetime.now() - timedelta(days=ANALYSIS_WINDOW_MONTHS * 30)\n",
    "cutoff_filter_str = cutoff_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"--- Início do Ciclo de Inferência ---\")\n",
    "print(f\"Data Base da Execução: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Janela configurada: Últimos {ANALYSIS_WINDOW_MONTHS} meses\")\n",
    "print(f\"Filtro de Data Calculado: >= {cutoff_filter_str}\")\n",
    "print(f\"Nota: A ingestão abaixo traz todos os dados novos (Incremental). O filtro deve ser usado na leitura dessas tabelas.\")\n",
    "# -----------------------------------------\n",
    "\n",
    "# Maping Origin to Destiny\n",
    "tables_config = { \n",
    "    SOURCE_PATH_1: (TARGET_TABLE_1, \"json\"),\n",
    "    SOURCE_PATH_2: (TARGET_TABLE_2, \"avro\"),\n",
    "    SOURCE_PATH_3: (TARGET_TABLE_3, \"json\"),\n",
    "    SOURCE_PATH_4: (TARGET_TABLE_4, \"avro\"),\n",
    "    SOURCE_PATH_5: (TARGET_TABLE_5, \"avro\")\n",
    "}\n",
    "\n",
    "CHECKPOINT_BASE_PATH = \"s3://aurorapay-studycase/checkpoints\"\n",
    "\n",
    "# Loop to process each table\n",
    "for source, (target, file_format) in tables_config.items():\n",
    "    print(f\"Lendo de: {source} ({file_format}) -> Gravando em: {target}\")\n",
    "\n",
    "    # Define checkpoint location in S3\n",
    "    table_name = target.split(\".\")[-1]\n",
    "    checkpoint_location = f\"{CHECKPOINT_BASE_PATH}/{table_name}\"\n",
    "    \n",
    "    # Pre-read to infer schema (Required for streaming non-Delta files like JSON/Avro)\n",
    "    try:\n",
    "        source_schema = spark.read.format(file_format).load(source).schema\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inferir schema para {source}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Loading only new data using Structured Streaming\n",
    "    (spark.readStream\n",
    "        .format(\"cloudFiles\") \n",
    "        .option(\"cloudFiles.format\", file_format)\n",
    "        .schema(source_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1000)\n",
    "        .load(source)\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8065854436121719,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "First_Stage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
